{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, datetime\n",
    "from contextlib import contextmanager\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.externals import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def measure_time(label):\n",
    "    \"\"\"\n",
    "    Context manager to measure time of computation.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    yield\n",
    "    end = time.time()\n",
    "    print('Duration of [{}]: {}'.format(label, datetime.timedelta(seconds=end-start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, to_split=True):\n",
    "    \"\"\"\n",
    "    Load the csv file and returns (X,y).\n",
    "    \"\"\"\n",
    "    # Read the csv file\n",
    "    df = pd.read_csv(path, header=0, index_col=0)\n",
    "\n",
    "    # Get the output values\n",
    "    y = df['crowd_class'].values.squeeze()\n",
    "\n",
    "    # Get the input values\n",
    "    feature = 'cluster'\n",
    "    X = df[feature].values.squeeze()\n",
    "    X = X.reshape(-1, 1)\n",
    "    \n",
    "    # Split train and test data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    if to_split:\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    else:\n",
    "        return X, y\n",
    "\n",
    "\n",
    "# Load the cleaned data\n",
    "path = './data/cleaned_HSL_data.csv'\n",
    "X_train, X_test, y_train, y_test = load_data(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration of [Training...]: 0:00:00.018964\n",
      "=================================================================\n",
      "Logistic Regression Training set accuracy: 0.3699217294073798\n",
      "=================================================================\n",
      "Logistic Regression Test set accuracy: 0.353204172876304\n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antoinelouis/.local/share/virtualenvs/Junction-2019-fvZjpDfB/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/antoinelouis/.local/share/virtualenvs/Junction-2019-fvZjpDfB/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def train(path, to_split=True):\n",
    "    \"\"\"\n",
    "    Train the model.\n",
    "    \"\"\"\n",
    "    filename = \"models/lin_reg.pkl\"\n",
    "    \n",
    "    # Load the training (and testing) set(s)\n",
    "    if to_split:\n",
    "        X_train, X_test, y_train, y_test = load_data(path, to_split=to_split)\n",
    "    else:\n",
    "        X_train, y_train = load_data(path, to_split=to_split)\n",
    "\n",
    "    with measure_time('Training...'):\n",
    "        model = LogisticRegression(random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        joblib.dump(model, filename) \n",
    "        \n",
    "    y_pred = model.predict(X_train)\n",
    "    print(\"=================================================================\")\n",
    "    print(\"Logistic Regression Training set accuracy: {}\".format(accuracy_score(y_train, y_pred)))\n",
    "    print(\"=================================================================\")\n",
    "    \n",
    "    \n",
    "    if to_split:\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(\"Logistic Regression Test set accuracy: {}\".format(accuracy_score(y_test, y_pred)))\n",
    "        print(\"=================================================================\")\n",
    "        \n",
    "        \n",
    "# Train our model\n",
    "train(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antoinelouis/.local/share/virtualenvs/Junction-2019-fvZjpDfB/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration of [Training...]: 0:00:00.533622\n",
      "=================================================================\n",
      "SVM Training set accuracy: 0.39619828550130454\n",
      "=================================================================\n",
      "SVM Test set accuracy: 0.36065573770491804\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "def train(path, to_split=True):\n",
    "    \"\"\"\n",
    "    Train the model.\n",
    "    \"\"\"\n",
    "    filename = \"models/svc.pkl\"\n",
    "    \n",
    "    # Load the training (and testing) set(s)\n",
    "    if to_split:\n",
    "        X_train, X_test, y_train, y_test = load_data(path, to_split=to_split)\n",
    "    else:\n",
    "        X_train, y_train = load_data(path, to_split=to_split)\n",
    "\n",
    "    with measure_time('Training...'):\n",
    "        model = SVC(kernel='rbf', max_iter=100000)\n",
    "        model.fit(X_train, y_train)\n",
    "        joblib.dump(model, filename) \n",
    "        \n",
    "    y_pred = svm_model.predict(X_train)\n",
    "    print(\"=================================================================\")\n",
    "    print(\"SVM Training set accuracy: {}\".format(accuracy_score(y_train, y_pred)))\n",
    "    print(\"=================================================================\")\n",
    "    \n",
    "    if to_split:\n",
    "        y_pred = svm_model.predict(X_test)\n",
    "        print(\"SVM Test set accuracy: {}\".format(accuracy_score(y_test, y_pred)))\n",
    "        print(\"=================================================================\")\n",
    "        \n",
    "        \n",
    "# Train our model\n",
    "train(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration of [Training...]: 0:00:00.018732\n",
      "=================================================================\n",
      "Knn Training set accuracy: 0.37700335445396943\n",
      "=================================================================\n",
      "Knn Test set accuracy: 0.36363636363636365\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "def train(path, to_split=True):\n",
    "    \"\"\"\n",
    "    Train the model.\n",
    "    \"\"\"\n",
    "    filename = \"models/knn.pkl\"\n",
    "    \n",
    "    # Load the training (and testing) set(s)\n",
    "    if to_split:\n",
    "        X_train, X_test, y_train, y_test = load_data(path, to_split=to_split)\n",
    "    else:\n",
    "        X_train, y_train = load_data(path, to_split=to_split)\n",
    "\n",
    "    with measure_time('Training...'):\n",
    "        model = KNeighborsClassifier(n_neighbors=100)\n",
    "        model.fit(X_train, y_train)\n",
    "        joblib.dump(model, filename) \n",
    "        \n",
    "    y_pred = model.predict(X_train)\n",
    "    print(\"=================================================================\")\n",
    "    print(\"Knn Training set accuracy: {}\".format(accuracy_score(y_train, y_pred)))\n",
    "    print(\"=================================================================\")\n",
    "    \n",
    "    if to_split:\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(\"Knn Test set accuracy: {}\".format(accuracy_score(y_test, y_pred)))\n",
    "        print(\"=================================================================\")\n",
    "        \n",
    "        \n",
    "# Train our model\n",
    "train(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:   21.3s\n",
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed:   49.0s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters {'momentum': 0.9, 'learning_rate_init': 0.1, 'learning_rate': 'adaptive', 'hidden_layer_sizes': (150,), 'activation': 'tanh'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "def tune_hyperparameter(path):\n",
    "    \"\"\"\n",
    "    Get the best hyperparameters.\n",
    "    \"\"\"\n",
    "   # Load the training set\n",
    "    X, y = load_data(path, to_split=False)\n",
    "        \n",
    "    # Create the random grid\n",
    "    random_grid = {'hidden_layer_sizes': [(20,), (50,), (100,), (150,)],\n",
    "                    'activation': ['tanh', 'relu', 'logistic', 'identity'],\n",
    "                    'learning_rate_init': [0.005, 0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "                    'learning_rate': ['constant','adaptive'],\n",
    "                    'momentum': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}\n",
    "    \n",
    "    # Use the random grid to search for best hyperparameters\n",
    "    # First create the base model to tune\n",
    "    mlp = MLPClassifier(solver='sgd', early_stopping=True)\n",
    "    # Random search of parameters, using 5 fold cross validation, \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    mlp_random = RandomizedSearchCV(estimator = mlp,\n",
    "                                   param_distributions = random_grid,\n",
    "                                   n_iter = 100,\n",
    "                                   cv = 5,\n",
    "                                   verbose=2,\n",
    "                                   random_state=42,\n",
    "                                   n_jobs = -1)\n",
    "    # Fit the random search model\n",
    "    mlp_random.fit(X, y)\n",
    "\n",
    "    print(\"Best parameters\", mlp_random.best_params_)\n",
    "    \n",
    "    \n",
    "tune_hyperparameter(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration of [Training...]: 0:00:00.665886\n",
      "=================================================================\n",
      "MLP Training set accuracy: 0.3740216175922475\n",
      "=================================================================\n",
      "MLP Test set accuracy: 0.36736214605067063\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "def create_estimator(path, to_split=True):\n",
    "    \"\"\"\n",
    "    Train the model.\n",
    "    \"\"\"\n",
    "    filename = \"models/mlp.pkl\"\n",
    "    \n",
    "    # Load the training (and testing) set(s)\n",
    "    if to_split:\n",
    "        X_train, X_test, y_train, y_test = load_data(path, to_split=to_split)\n",
    "    else:\n",
    "        X_train, y_train = load_data(path, to_split=to_split)\n",
    "\n",
    "    with measure_time('Training...'):\n",
    "        model = MLPClassifier(solver='sgd', \n",
    "                                hidden_layer_sizes = (15,), \n",
    "                                early_stopping=True,\n",
    "                                learning_rate_init= 0.1,\n",
    "                                learning_rate = 'adaptive',\n",
    "                                activation='tanh',\n",
    "                                momentum=0.9)\n",
    "        model.fit(X_train, y_train)\n",
    "        joblib.dump(model, filename) \n",
    "        \n",
    "    y_pred = model.predict(X_train)\n",
    "    print(\"=================================================================\")\n",
    "    print(\"MLP Training set accuracy: {}\".format(accuracy_score(y_train, y_pred)))\n",
    "    print(\"=================================================================\")\n",
    "    \n",
    "    if to_split:\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(\"MLP Test set accuracy: {}\".format(accuracy_score(y_test, y_pred)))\n",
    "        print(\"=================================================================\")\n",
    "\n",
    "\n",
    "create_estimator(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
