{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Create a workspace"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import azureml.core\n\nprint(azureml.core.VERSION)\n\nfrom azureml.core import Workspace\n\nws = Workspace.create(name='GoGreenworkspace',\n            subscription_id='2bf014a6-b217-4034-a315-cb95042c9087', \n            resource_group='rgGoGreenworkspace',\n            create_resource_group = True,\n            location='North Europe'\n            )\n\nprint('AMLS Workspace created')",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": "1.0.72\nDeploying KeyVault with name gogreenwkeyvault6cf17edb.\nDeploying StorageAccount with name gogreenwstorage8897f03af.\nDeploying AppInsights with name gogreenwinsights0b3ecb09.\nDeployed AppInsights with name gogreenwinsights0b3ecb09. Took 9.45 seconds.\nDeployed KeyVault with name gogreenwkeyvault6cf17edb. Took 21.4 seconds.\nDeployed StorageAccount with name gogreenwstorage8897f03af. Took 27.56 seconds.\nDeploying Workspace with name GoGreenworkspace.\nDeployed Workspace with name GoGreenworkspace. Took 135.54 seconds.\nAMLS Workspace created\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Create the configuration file"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "ws.write_config(path=\"./config\", file_name=\"ws_config.json\")\nprint('Configuration saved')",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Configuration saved\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Create a remote compute target"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.compute import AmlCompute\nfrom azureml.core.compute import ComputeTarget\nimport os\n\n# Step 1: name the cluster and set the minimal and maximal number of nodes \ncompute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"cpucluster\")\nmin_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\nmax_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 3)\n\n# Step 2: choose environment variables \nvm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D2_V2\")\n\nprovisioning_config = AmlCompute.provisioning_configuration(\n    vm_size = vm_size, min_nodes = min_nodes, max_nodes = max_nodes)\n\n# create the cluster\ncompute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n\nprint('Compute target created')",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Compute target created\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Training scripts"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\n\n# create the folder of the training Python scripts\nfolder_training_script = './training_scripts'\nos.makedirs(folder_training_script, exist_ok=True)\n\nprint('Done')",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Done\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Support Vector Machine"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile $folder_training_script/train_SVC.py\n\nimport time, datetime\nfrom contextlib import contextmanager\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.externals import joblib\nfrom sklearn.svm import SVC\n\n\n@contextmanager\ndef measure_time(label):\n    \"\"\"\n    Context manager to measure time of computation.\n    \"\"\"\n    start = time.time()\n    yield\n    end = time.time()\n    print('Duration of [{}]: {}'.format(label, datetime.timedelta(seconds=end-start)))\n\n\ndef load_data(path, to_split=True):\n    \"\"\"\n    Load the csv file and returns (X,y).\n    \"\"\"\n    # Read the csv file\n    df = pd.read_csv(path, header=0, index_col=0)\n\n    # Get the output values\n    y = df['crowd_class'].values.squeeze()\n\n    # Get the input values\n    feature = 'cluster'\n    X = df[feature].values.squeeze()\n    X = X.reshape(-1, 1)\n    \n    # Split train and test data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    if to_split:\n        return X_train, X_test, y_train, y_test\n    else:\n        return X, y\n    \n    \ndef train(path, to_split=True):\n    \"\"\"\n    Train the model.\n    \"\"\"\n    filename = \"../models/svc.pkl\"\n    \n    # Load the training (and testing) set(s)\n    if to_split:\n        X_train, X_test, y_train, y_test = load_data(path, to_split=to_split)\n    else:\n        X_train, y_train = load_data(path, to_split=to_split)\n\n    with measure_time('Training...'):\n        model = SVC(kernel='rbf', max_iter=100000)\n        model.fit(X_train, y_train)\n        joblib.dump(model, filename) \n        \n    y_pred = model.predict(X_train)\n    print(\"=================================================================\")\n    print(\"SVM Training set accuracy: {}\".format(accuracy_score(y_train, y_pred)))\n    print(\"=================================================================\")\n    \n    if to_split:\n        y_pred = model.predict(X_test)\n        print(\"SVM Test set accuracy: {}\".format(accuracy_score(y_test, y_pred)))\n        print(\"=================================================================\")\n\n\n# Train our model\npath = './data/cleaned_HSL_data.csv'\ntrain(path)",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Overwriting ./training_scripts/train_SVC.py\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Logistic Regression"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile $folder_training_script/train_logreg.py\n\nimport time, datetime\nfrom contextlib import contextmanager\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.externals import joblib\nfrom sklearn.linear_model import LogisticRegression\n\n\n@contextmanager\ndef measure_time(label):\n    \"\"\"\n    Context manager to measure time of computation.\n    \"\"\"\n    start = time.time()\n    yield\n    end = time.time()\n    print('Duration of [{}]: {}'.format(label, datetime.timedelta(seconds=end-start)))\n\n\ndef load_data(path, to_split=True):\n    \"\"\"\n    Load the csv file and returns (X,y).\n    \"\"\"\n    # Read the csv file\n    df = pd.read_csv(path, header=0, index_col=0)\n\n    # Get the output values\n    y = df['crowd_class'].values.squeeze()\n\n    # Get the input values\n    feature = 'cluster'\n    X = df[feature].values.squeeze()\n    X = X.reshape(-1, 1)\n    \n    # Split train and test data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    if to_split:\n        return X_train, X_test, y_train, y_test\n    else:\n        return X, y\n    \n    \ndef train(path, to_split=True):\n    \"\"\"\n    Train the model.\n    \"\"\"\n    filename = \"../models/lin_reg.pkl\"\n    \n    # Load the training (and testing) set(s)\n    if to_split:\n        X_train, X_test, y_train, y_test = load_data(path, to_split=to_split)\n    else:\n        X_train, y_train = load_data(path, to_split=to_split)\n\n    with measure_time('Training...'):\n        model = LogisticRegression(random_state=42)\n        model.fit(X_train, y_train)\n        joblib.dump(model, filename) \n        \n    y_pred = model.predict(X_train)\n    print(\"=================================================================\")\n    print(\"Logistic Regression Training set accuracy: {}\".format(accuracy_score(y_train, y_pred)))\n    print(\"=================================================================\")\n    \n    \n    if to_split:\n        y_pred = model.predict(X_test)\n        print(\"Logistic Regression Test set accuracy: {}\".format(accuracy_score(y_test, y_pred)))\n        print(\"=================================================================\")\n\n\n# Train our model\npath = './data/cleaned_HSL_data.csv'\ntrain(path)",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Overwriting ./training_scripts/train_logreg.py\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## K-Nearest Neighbors"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile $folder_training_script/train_knn.py\n\nimport time, datetime\nfrom contextlib import contextmanager\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.externals import joblib\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\n@contextmanager\ndef measure_time(label):\n    \"\"\"\n    Context manager to measure time of computation.\n    \"\"\"\n    start = time.time()\n    yield\n    end = time.time()\n    print('Duration of [{}]: {}'.format(label, datetime.timedelta(seconds=end-start)))\n\n\ndef load_data(path, to_split=True):\n    \"\"\"\n    Load the csv file and returns (X,y).\n    \"\"\"\n    # Read the csv file\n    df = pd.read_csv(path, header=0, index_col=0)\n\n    # Get the output values\n    y = df['crowd_class'].values.squeeze()\n\n    # Get the input values\n    feature = 'cluster'\n    X = df[feature].values.squeeze()\n    X = X.reshape(-1, 1)\n    \n    # Split train and test data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    if to_split:\n        return X_train, X_test, y_train, y_test\n    else:\n        return X, y\n    \n    \ndef train(path, to_split=True):\n    \"\"\"\n    Train the model.\n    \"\"\"\n    filename = \"../models/knn.pkl\"\n    \n    # Load the training (and testing) set(s)\n    if to_split:\n        X_train, X_test, y_train, y_test = load_data(path, to_split=to_split)\n    else:\n        X_train, y_train = load_data(path, to_split=to_split)\n\n    with measure_time('Training...'):\n        model = KNeighborsClassifier(n_neighbors=100)\n        model.fit(X_train, y_train)\n        joblib.dump(model, filename) \n        \n    y_pred = model.predict(X_train)\n    print(\"=================================================================\")\n    print(\"Knn Training set accuracy: {}\".format(accuracy_score(y_train, y_pred)))\n    print(\"=================================================================\")\n    \n    if to_split:\n        y_pred = model.predict(X_test)\n        print(\"Knn Test set accuracy: {}\".format(accuracy_score(y_test, y_pred)))\n        print(\"=================================================================\")\n\n\n# Train our model\npath = './data/cleaned_HSL_data.csv'\ntrain(path)",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Writing ./training_scripts/train_knn.py\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Multi-Layer Perceptron"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile $folder_training_script/train_mlp.py\n\nimport time, datetime\nfrom contextlib import contextmanager\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.externals import joblib\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\n\n@contextmanager\ndef measure_time(label):\n    \"\"\"\n    Context manager to measure time of computation.\n    \"\"\"\n    start = time.time()\n    yield\n    end = time.time()\n    print('Duration of [{}]: {}'.format(label, datetime.timedelta(seconds=end-start)))\n\n\ndef load_data(path, to_split=True):\n    \"\"\"\n    Load the csv file and returns (X,y).\n    \"\"\"\n    # Read the csv file\n    df = pd.read_csv(path, header=0, index_col=0)\n\n    # Get the output values\n    y = df['crowd_class'].values.squeeze()\n\n    # Get the input values\n    feature = 'cluster'\n    X = df[feature].values.squeeze()\n    X = X.reshape(-1, 1)\n    \n    # Split train and test data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    if to_split:\n        return X_train, X_test, y_train, y_test\n    else:\n        return X, y\n    \n    \ndef tune_hyperparameter(path):\n    \"\"\"\n    Get the best hyperparameters.\n    \"\"\"\n   # Load the training set\n    X, y = load_data(path, to_split=False)\n        \n    # Create the random grid\n    random_grid = {'hidden_layer_sizes': [(20,), (50,), (100,), (150,)],\n                    'activation': ['tanh', 'relu', 'logistic', 'identity'],\n                    'learning_rate_init': [0.005, 0.01, 0.05, 0.1, 0.2, 0.3],\n                    'learning_rate': ['constant','adaptive'],\n                    'momentum': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}\n    \n    # Use the random grid to search for best hyperparameters\n    # First create the base model to tune\n    mlp = MLPClassifier(solver='sgd', early_stopping=True)\n    # Random search of parameters, using 5 fold cross validation, \n    # search across 100 different combinations, and use all available cores\n    mlp_random = RandomizedSearchCV(estimator = mlp,\n                                   param_distributions = random_grid,\n                                   n_iter = 100,\n                                   cv = 5,\n                                   verbose=2,\n                                   random_state=42,\n                                   n_jobs = -1)\n    # Fit the random search model\n    mlp_random.fit(X, y)\n\n    # Return optimal parameters\n    return mlp_random.best_params_\n    \n    \ndef create_estimator(path, params, to_split=True):\n    \"\"\"\n    Train the model.\n    \"\"\"\n    filename = \"models/mlp.pkl\"\n    \n    # Load the training (and testing) set(s)\n    if to_split:\n        X_train, X_test, y_train, y_test = load_data(path, to_split=to_split)\n    else:\n        X_train, y_train = load_data(path, to_split=to_split)\n        \n    # Extract parameters\n    hidden_layer_sizes = params['hidden_layer_sizes']\n    learning_rate_init = params['learning_rate_init']\n    learning_rate = params['learning_rate']\n    activation = params['activation']\n    momentum = params['momentum']\n\n    with measure_time('Training...'):\n        model = MLPClassifier(solver='sgd', \n                                hidden_layer_sizes=hidden_layer_sizes, \n                                early_stopping=True,\n                                learning_rate_init=learning_rate_init,\n                                learning_rate = learning_rate,\n                                activation= activation,\n                                momentum= momentum)\n        model.fit(X_train, y_train)\n        joblib.dump(model, filename) \n        \n    y_pred = model.predict(X_train)\n    print(\"=================================================================\")\n    print(\"MLP Training set accuracy: {}\".format(accuracy_score(y_train, y_pred)))\n    print(\"=================================================================\")\n    \n    if to_split:\n        y_pred = model.predict(X_test)\n        print(\"MLP Test set accuracy: {}\".format(accuracy_score(y_test, y_pred)))\n        print(\"=================================================================\")\n\n\n\n# Hypertune model\npath = './data/cleaned_HSL_data.csv'\nparams = tune_hyperparameter(path)\nprint(\"Best parameters\", params)\n\n# Train model on best parameters\ncreate_estimator(path, params)",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Writing ./training_scripts/train_mlp.py\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Run code on Azure"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.train.sklearn import SKLearn\nfrom azureml.core import Experiment\n\n\n#import the Scikit-learn package \nest = SKLearn(source_directory=folder_training_script,\n                compute_target=compute_target,\n                entry_script='train_SVC.py',\n                conda_packages=['scikit-learn'])\n\n#Create an experiment\nexperiment = Experiment(workspace = ws, name = \"gogreen-experiment\")\nprint('Experiment created')\n\n\n# Run the experiment\nrun = experiment.submit(config=est)\nrun",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": "WARNING - You have specified to install packages in your run. Note that you have overridden Azure ML's installation of the following packages: ['scikit-learn']. We cannot guarantee image build will succeed.\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Experiment created\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 53,
          "data": {
            "text/html": "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>gogreen-experiment</td><td>gogreen-experiment_1573945753_53351dc1</td><td>azureml.scriptrun</td><td>Starting</td><td><a href=\"https://ml.azure.com/experiments/gogreen-experiment/runs/gogreen-experiment_1573945753_53351dc1?wsid=/subscriptions/2bf014a6-b217-4034-a315-cb95042c9087/resourcegroups/rgGoGreenworkspace/workspaces/GoGreenworkspace\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>",
            "text/plain": "Run(Experiment: gogreen-experiment,\nId: gogreen-experiment_1573945753_53351dc1,\nType: azureml.scriptrun,\nStatus: Starting)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}